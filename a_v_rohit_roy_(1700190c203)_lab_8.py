# -*- coding: utf-8 -*-
"""A V Rohit Roy (1700190C203) lab_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mzAH3e-svjtgV9AtQb4ut0qE5XDBTDN0
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, log_loss, classification_report)
from imblearn.over_sampling import SMOTE

#Note:The target variable of given dataset is Attrition...
#Reading the values..
attrition = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')
attrition.head()

attrition.shape

# Looking for NULL values..
display(attrition.isnull().any())

attrition.Attrition.value_counts().plot(kind='bar', color="green", alpha=.45)
plt.title("Attrition Breakdown")

"""The dataset is imbalanced one so we have to apply oversampling technique smote on latter stages."""

#Breakdown of attricutes with respect to gender...
pd.crosstab(attrition.Gender,attrition.Attrition).plot(kind='bar')
plt.title('Attrition with respect to Gender')
plt.xlabel('Gender')
plt.ylabel('Frequency')

"""It seems to be more of male employees have been attritioned than female employees.so gender may have some effect on attrition."""

#Breakdown with respect to Department..
pd.crosstab(attrition.Department,attrition.Attrition).plot(kind='bar', stacked=False)
plt.title('Attrition with respect to Department')
plt.xlabel('Department')
plt.ylabel('Frequency of Attrition')

"""Department variable has a huge effect on attrition varible and this will be useful for modelling and prediction. When compared to R&D the employees of HR and Sales are comperatively low."""

#Breakdown with respect to Business Travel
pd.crosstab(attrition.BusinessTravel,attrition.Attrition).plot(kind='bar')
plt.title('Attrition with respect to BusinessTravel')
plt.xlabel('BusinessTravel')
plt.ylabel('Frequency of Attrition')
plt.xticks(rotation=40)

"""Irrespective of the business travel the attrition is less."""

pd.crosstab(attrition['JobRole'],attrition['Attrition']).plot(kind='bar', stacked=False)
plt.title('Attrition with respect to JobRole')
plt.xlabel('JobRole')
plt.ylabel('Frequency of Attrition')

"""Job role seems to have an impact on the attrition rate of employees based on the above plot and we see less attrition in the case of Research Director"""

#Breakdown with respect to Education field...
pd.crosstab(attrition.EducationField,attrition.Attrition).plot(kind='bar',stacked=False)
plt.title('Attrition with respect to EducationField')
plt.xlabel('EducationField')
plt.ylabel('Frequency of Attrition')

"""LifeSciences and Medical has the large number of employees and attrition rate also.The % of employees who have been attritioned and who have been retained are also same in all education fields."""

#Breakdown with respect to JobLevel...
pd.crosstab(attrition.JobLevel,attrition.Attrition).plot(kind='bar',stacked=False)
plt.title('Attrition with respect to JobLevel')
plt.xlabel('JobLevel')
plt.ylabel('Frequency of Attrition')

"""People in Joblevel 4 have a very high percent for a 'No' and a low percent for a 'Yes'. Similar inferences can be made for other job levels."""

#Breakdown with respect to JobSatisfication...
pd.crosstab(attrition.JobSatisfaction,attrition.Attrition).plot(kind='bar',stacked=False)
plt.title('Attrition with respect to JobSatisfication')
plt.xlabel('JobSatisfication')
plt.ylabel('Frequency of Attrition')

"""for higher values of job satisfaction(i.e. more a person is satisfied with his job) lesser percent of them say a 'Yes' which is quite obvious as highly contented workers will obvioulsy not like to leave the organisation."""

#Breakdown with respect to Environment Satisfication...
pd.crosstab(attrition.EnvironmentSatisfaction,attrition.Attrition).plot(kind='bar',stacked=False)
plt.title('Attrition with respect to Environment Satisfication')
plt.xlabel('Environment Satisfication')
plt.ylabel('Frequency of Attrition')

"""we can notice that the relative percent of 'No' in people with higher grade of environment satisfacftion. This can be done for all the features further but i feel this are some of the important features,which can be used aganist our target class"""

wcorr = attrition.corr()
plt.matshow(wcorr.abs())
plt.colorbar()
plt.xticks(range(len(wcorr.columns)), wcorr.columns, rotation='vertical');
plt.yticks(range(len(wcorr.columns)), wcorr.columns);

""".corr() method on a pandas data frame returns a corelation data frame containing the corelation values b/w the various attributes.we can drop some highly corelated features as they add redundancy to the model but since the corelation is very less in genral let us keep all the features for now. In case of highly corelated features we can use something like Principal Component Analysis(PCA) to reduce our feature space."""

attrition_tar= attrition.drop(['Attrition'], axis=1)
y=attrition['Attrition']

"""Here we have changed the target label which of categorical data type into numerical data type"""

# Import the train_test_split method
from sklearn.model_selection import train_test_split

# Split data into train and test sets as well as for validation and testing
x_train, x_test, y_train, y_test = train_test_split(attrition_tar,y,train_size= 0.80,random_state=0);
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""Feature Encoding and Categorical Encoding."""

def encoding(data):
    # Empty list to store columns with categorical data
    categorical = []
    for col, value in data.iteritems():
        if value.dtype == 'object':
            categorical.append(col)

    # Store the numerical columns in a list numerical
    numerical = data.columns.difference(categorical)
    attrition_cat = data[categorical]
    attrition_cat = pd.get_dummies(attrition_cat)
    # Store the numerical features to a dataframe attrition_num
    attrition_num = data[numerical]
    # Concat the two dataframes together columnwise
    attrition_final = pd.concat([attrition_num, attrition_cat], axis=1)
    print(attrition_final.columns)
    return attrition_final

x_train_encode=encoding(x_train)
x_train_encode.shape

x_test_encode=encoding(x_test)
x_test_encode.shape

"""Here we divided our dataset into numerical and categorical,and we store it in a list.and for every categorical value we have created a dummy values for example,Business travel(Non-travel,Travel-frequently,travelRarely)...."""

def target_encode(data):
    # Define a dictionary for the target mapping
    target_map = {'Yes':1, 'No':0}
# Use the pandas apply method to numerically encode our attrition target variable
    target = data.apply(lambda x: target_map[x])
    return target

y_train_encode=target_encode(y_train)
y_test_encode=target_encode(y_test)

"""Now we encoded the target class Attrition with values of 1 and 0 by replacing the yes/no."""

#Since we have noticed severe imbalance at the beginning we now use SMOTE method to reduce the imbalances.
oversampler=SMOTE(random_state=0)
smote_train, smote_target = oversampler.fit_sample(x_train_encode,y_train_encode)

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
logr = LogisticRegression(max_iter=1000)
logr.fit(smote_train, smote_target)
y_pred = logr.predict(x_test_encode)
print("Accuracy score: {}".format(accuracy_score(y_test_encode, y_pred)))
print("="*80)
print(classification_report(y_test_encode, y_pred))

"""**Random Forest**"""

seed = 0   # We set our random seed to zero for reproducibility
# Random Forest parameters
rf_params = {
    'n_jobs': -1,
    'n_estimators': 1000,
#     'warm_start': True, 
    'max_features': 0.3,
    'max_depth': 4,
    'min_samples_leaf': 2,
    'max_features' : 'sqrt',
    'random_state' : seed,
    'verbose': 0
}

rf = RandomForestClassifier(**rf_params)
rf.fit(smote_train, smote_target)
print("Fitting of Random Forest finished")

rf_predictions = rf.predict(x_test_encode)
print("Predictions finished")

print("Accuracy score: {}".format(accuracy_score(y_test_encode, rf_predictions)))
print("="*80)
print(classification_report(y_test_encode, rf_predictions))

"""The Random Forest classifier in Sklearn contains a very convenient attribute featureimportances which tells us which features within our dataset has been given most importance through the Random Forest algorithm. And the Accuracy of Random Forest Model is **85.3%**."""

#Shown below is an Interactive Plotly diagram of the various feature importances.

features = x_train_encode.columns.values
importances = rf.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(15,15))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='g', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""**Decision Tree:**"""

from sklearn import tree
dt = tree.DecisionTreeClassifier()
dt.fit(smote_train, smote_target)
print("Fitting of Random Forest finished")

dt_predictions = dt.predict(x_test_encode)
print("Predictions finished")

print("Accuracy score: {}".format(accuracy_score(y_test_encode, dt_predictions)))
print("="*80)
print(classification_report(y_test_encode, dt_predictions))

"""The Accuracy of Decision Tree is around **74.5%**."""

features = x_train_encode.columns.values
importances = dt.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(15,15))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='r', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""We have predicted employee attrition,from the all the 3 models implemented Random Forest turnsout to be best model of all."""